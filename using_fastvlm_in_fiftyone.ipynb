{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FastVLM Models with FiftyOne\n",
        "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/fast_vlm/blob/main/using_fastvlm_in_fiftyone.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use Apple's FastVLM models for visual question answering and creative tasks using FiftyOne.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install fiftyone torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.huggingface as fouh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register and Download FastVLM Model\n",
        "\n",
        "We'll use the 1.5B parameter model for this example as it provides a good balance between performance and resource usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the model source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/fast_vlm\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "# Download the model (first time only)\n",
        "foz.download_zoo_model(\n",
        "    \"https://github.com/harpreetsahota204/fast_vlm\",\n",
        "    model_name=\"apple/FastVLM-0.5B\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Sample Dataset\n",
        "\n",
        "We'll use the MashUpVQA dataset from HuggingFace, which contains diverse images with questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a small subset for demonstration\n",
        "dataset = fouh.load_from_hub(\n",
        "    \"Voxel51/MashUpVQA\",\n",
        "    max_samples=10,\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Visual Question Answering\n",
        "\n",
        "Let's start with a simple example where we ask the same question for all images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with a default prompt\n",
        "model = foz.load_zoo_model(\n",
        "    \"apple/FastVLM-1.5B\",\n",
        ")\n",
        "\n",
        "model.prompt=\"Describe the main activity or event happening in this image.\"\n",
        "\n",
        "# Apply to dataset\n",
        "dataset.apply_model(model, label_field=\"activity_description\")\n",
        "\n",
        "# View a sample result\n",
        "sample = dataset.first()\n",
        "print(\"Sample Image Description:\")\n",
        "print(sample.activity_description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Using Dataset Questions\n",
        "\n",
        "Now let's use the questions that come with the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use questions from the dataset\n",
        "dataset.apply_model(\n",
        "    model,\n",
        "    prompt_field=\"question\",\n",
        "    label_field=\"model_answer\"\n",
        ")\n",
        "\n",
        "# View sample Q&A\n",
        "sample = dataset.first()\n",
        "print(\"Question:\", sample.question)\n",
        "print(\"Answer:\", sample.model_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Creative Generation\n",
        "\n",
        "FastVLM can also generate creative content based on images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure model for creative generation\n",
        "model.prompt = \"Write a short, creative poem about what you see in this image.\"\n",
        "model.temperature = 0.9  # Increase creativity\n",
        "model.max_new_tokens = 100  # Allow longer responses\n",
        "\n",
        "# Generate poems\n",
        "dataset.apply_model(model, label_field=\"poem\")\n",
        "\n",
        "# View a sample poem\n",
        "sample = dataset.first()\n",
        "print(\"Generated Poem:\")\n",
        "print(sample.poem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Detailed Scene Analysis\n",
        "\n",
        "Let's use a structured prompt to get detailed scene analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure model for detailed analysis\n",
        "model.prompt = \"\"\"\n",
        "Analyze this image and provide:\n",
        "1. Main subjects/objects\n",
        "2. Actions/activities\n",
        "3. Setting/environment\n",
        "4. Notable details\n",
        "5. Overall mood/atmosphere\n",
        "\"\"\".strip()\n",
        "\n",
        "# Generate analysis\n",
        "dataset.apply_model(model, label_field=\"detailed_analysis\")\n",
        "\n",
        "# View sample analysis\n",
        "sample = dataset.first()\n",
        "print(\"Detailed Analysis:\")\n",
        "print(sample.detailed_analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Results\n",
        "\n",
        "Launch the FiftyOne App to interactively explore all results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#install caption viewer plugin:\n",
        "\n",
        "!fiftyone plugins download https://github.com/mythrandire/caption-viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Close the FiftyOne App session when done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fiftyone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
